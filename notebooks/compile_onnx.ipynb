{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "592e4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import relay\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tvm.contrib.download import download_testdata\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c02f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote labels.txt with 1000 classes\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "labels = ResNet18_Weights.DEFAULT.meta[\"categories\"]  # 1000 labels\n",
    "with open(\"labels.txt\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(labels))\n",
    "print(\"Wrote labels.txt with\", len(labels), \"classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c622904",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"resnet18\"\n",
    "model = getattr(torchvision.models, model_name)(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# We grab the TorchScripted model via tracing\n",
    "input_shape = [1, 3, 224, 224]\n",
    "input_data = torch.randn(input_shape)\n",
    "scripted_model = torch.jit.trace(model, input_data).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f289b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\n",
    "img_path = download_testdata(img_url, \"cat.png\", module=\"data\")\n",
    "img = Image.open(img_path).resize((224, 224))\n",
    "\n",
    "# Preprocess the image and convert to tensor\n",
    "from torchvision import transforms\n",
    "\n",
    "my_preprocess = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "img = my_preprocess(img)\n",
    "img = np.expand_dims(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3aafda",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = \"input0\"\n",
    "shape_list = [(input_name, img.shape)]\n",
    "mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef5ca1",
   "metadata": {},
   "source": [
    "## Choosing the Target Architecture\n",
    "\n",
    "In TVM, a target defines how the model will be compiled for the hardware. You can either use:\n",
    "\n",
    "1. **Target strings**: e.g., `\"llvm -mtriple=aarch64-linux-gnu -mcpu=cortex-a72 -mattr=+neon\"`\n",
    "2. **Helper functions**: e.g., `tvm.target.arm_cpu(model=\"raspi4\")` or `tvm.target.cuda()`\n",
    "\n",
    "| Device | TVM Helper | Example Target String |\n",
    "|--------|------------|---------------------|\n",
    "| Raspberry Pi 4 (64-bit) | `tvm.target.arm_cpu(model=\"raspi4\")` | `llvm -mtriple=aarch64-linux-gnu -mcpu=cortex-a72 -mattr=+neon` |\n",
    "| Raspberry Pi 3 / Zero 2 | `tvm.target.arm_cpu(model=\"raspi3\")` | `llvm -mtriple=aarch64-linux-gnu -mcpu=cortex-a53 -mattr=+neon` |\n",
    "| x86_64 Desktop | `tvm.target.llvm(mcpu=\"native\")` | `llvm -mcpu=native` |\n",
    "| Jetson Xavier NX GPU | `tvm.target.cuda(arch=\"sm_75\")` | `cuda -arch=sm_75` |\n",
    "| STM32 Microcontrollers | `tvm.target.stm32(series=\"stm32H7xx\")` | `c -mcpu=cortex-m7` |\n",
    "\n",
    "### Cross-Compilation Notes\n",
    "\n",
    "When compiling on a host machine (e.g., x86 laptop) for an embedded target (Raspberry Pi, Jetson), \n",
    "TVM uses cross-compilation. Make sure to:\n",
    "\n",
    "- Specify `mtriple` to match target OS/architecture.\n",
    "- Specify `mcpu` to match the exact CPU core.\n",
    "- Optionally specify `mattr` for hardware features.\n",
    "\n",
    "You can list built-in target tags using:\n",
    "```python\n",
    "tvm.target.list_tags()\n",
    "```\n",
    "\n",
    "Refs: https://tvm.apache.org/docs/reference/api/python/target.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79ce6683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n"
     ]
    }
   ],
   "source": [
    "target = tvm.target.Target(\"llvm\", host=\"llvm\")\n",
    "\n",
    "dev = tvm.cpu(0)\n",
    "with tvm.transform.PassContext(opt_level=3):\n",
    "    lib = relay.build(mod, target=target, params=params)\n",
    "\n",
    "# Save graph JSON\n",
    "with open(\"resnet18_tvm.json\", \"w\") as f_json:\n",
    "    f_json.write(lib.get_graph_json())\n",
    "\n",
    "# Save parameters\n",
    "with open(\"resnet18_tvm.params\", \"wb\") as f_params:\n",
    "    f_params.write(tvm.runtime.save_param_dict(lib.params))\n",
    "\n",
    "# Compile into a shared lib, dynamic lib if your slang is from windows ;)\n",
    "from tvm.contrib import cc\n",
    "lib.export_library(\"resnet18_tvm.so\", fcompile=cc.create_shared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952dea1c",
   "metadata": {},
   "source": [
    "## After compilation\n",
    "\n",
    "After compiling the model to your preffered architecture, you can continue with this notebook to run the model in python (your target architecture or here)\n",
    "or you can go to the `tvm_cpp` directory and paste into `artifacts` the following files generated by TVM:\n",
    "- labels.txt\n",
    "- resnet18_tvm.json\n",
    "- resnet18_tvm.params\n",
    "- resnet18_tvm.so\n",
    "\n",
    "From this you can compile your C++ code with the appropriate libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5027bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.contrib import graph_executor\n",
    "\n",
    "dtype = \"float32\"\n",
    "m = graph_executor.GraphModule(lib[\"default\"](dev))\n",
    "# Set inputs\n",
    "m.set_input(input_name, tvm.nd.array(img.astype(dtype)))\n",
    "# Execute\n",
    "m.run()\n",
    "# Get outputs\n",
    "tvm_output = m.get_output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4fc3452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relay top-1 id: 281, class name: tabby, tabby cat\n",
      "Torch top-1 id: 281, class name: tabby, tabby cat\n"
     ]
    }
   ],
   "source": [
    "synset_url = \"\".join(\n",
    "    [\n",
    "        \"https://raw.githubusercontent.com/Cadene/\",\n",
    "        \"pretrained-models.pytorch/master/data/\",\n",
    "        \"imagenet_synsets.txt\",\n",
    "    ]\n",
    ")\n",
    "synset_name = \"imagenet_synsets.txt\"\n",
    "synset_path = download_testdata(synset_url, synset_name, module=\"data\")\n",
    "with open(synset_path) as f:\n",
    "    synsets = f.readlines()\n",
    "\n",
    "synsets = [x.strip() for x in synsets]\n",
    "splits = [line.split(\" \") for line in synsets]\n",
    "key_to_classname = {spl[0]: \" \".join(spl[1:]) for spl in splits}\n",
    "\n",
    "class_url = \"\".join(\n",
    "    [\n",
    "        \"https://raw.githubusercontent.com/Cadene/\",\n",
    "        \"pretrained-models.pytorch/master/data/\",\n",
    "        \"imagenet_classes.txt\",\n",
    "    ]\n",
    ")\n",
    "class_name = \"imagenet_classes.txt\"\n",
    "class_path = download_testdata(class_url, class_name, module=\"data\")\n",
    "with open(class_path) as f:\n",
    "    class_id_to_key = f.readlines()\n",
    "\n",
    "class_id_to_key = [x.strip() for x in class_id_to_key]\n",
    "\n",
    "# Get top-1 result for TVM\n",
    "top1_tvm = np.argmax(tvm_output.numpy()[0])\n",
    "tvm_class_key = class_id_to_key[top1_tvm]\n",
    "\n",
    "# Convert input to PyTorch variable and get PyTorch result for comparison\n",
    "with torch.no_grad():\n",
    "    torch_img = torch.from_numpy(img)\n",
    "    output = model(torch_img)\n",
    "\n",
    "    # Get top-1 result for PyTorch\n",
    "    top1_torch = np.argmax(output.numpy())\n",
    "    torch_class_key = class_id_to_key[top1_torch]\n",
    "\n",
    "print(\"Relay top-1 id: {}, class name: {}\".format(top1_tvm, key_to_classname[tvm_class_key]))\n",
    "print(\"Torch top-1 id: {}, class name: {}\".format(top1_torch, key_to_classname[torch_class_key]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
